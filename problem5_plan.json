{
  "title": "Problem 5 Plan - Solving a Real-World Problem Using Reinforcement Learning (Q-learning on FrozenLake)",
  "summary": "Step-by-step walkthrough and implementation plan for Problem 5: Warehouse Robot on a Slippery Floor using tabular Q-learning and the FrozenLake-v1 environment.",
  "contract": {
  "inputs": ["Gymnasium environment: FrozenLake-v1 (4x4 or 8x8, slippery=True)", "Hyperparameters: alpha, gamma, epsilon/decay, episodes, max_steps_per_episode"],
    "outputs": ["Trained Q-table (n_states x n_actions)", "Evaluation metrics: success rate, average steps, reward history, stability across seeds", "Plots: learning curve, policy map, Q-value heatmap", "README and report sections describing methods and results"],
    "error_modes": ["Environment API mismatch (gym vs gymnasium)", "Insufficient training (convergence not reached)", "High variance due to stochastic transitions"],
    "success_criteria": ["Agent reaches an acceptable success rate in evaluation (recommend 70%+ for 4x4, 50%+ for harder maps) ", "Learning curve shows improvement vs episodes", "Reproducible results for fixed seeds"]
  },
  "assumptions": [
    "Python 3.8+ available",
    "Use of OpenAI Gym's FrozenLake-v1 or Gymnasium equivalent",
    "Tabular method is acceptable (state space small enough for table)",
    "The user will run experiments locally and can install required packages"
  ],
  "steps": [
    {
      "id": 1,
      "title": "Project scaffold and environment setup",
      "tasks": [
        "Create a single Jupyter notebook `notebooks/problem5_runner.ipynb` containing one code cell with runnable code to instantiate FrozenLake-v1 and run a minimal train/evaluate flow",
        "Add a minimal requirements.txt listing: gymnasium, numpy, matplotlib, seaborn",
        "Create a README.md with high-level run instructions and this plan summary"
      ],
      "files": ["notebooks/problem5_runner.ipynb", "requirements.txt", "README.md"],
  "expected_output": "A runnable notebook that can import gymnasium and numpy and demonstrate instantiating FrozenLake-v1 and a minimal train/evaluate flow",
      "time_estimate_minutes": 20,
  "verification": "python -c \"import gymnasium as gym; env = gym.make('FrozenLake-v1'); print(env.observation_space, env.action_space)\" (or equivalent)"
    },
    {
      "id": 2,
      "title": "Understanding the environment",
      "tasks": [
        "Instantiate FrozenLake-v1 with map size parameter (e.g., '4x4' or '8x8') and slippery=True",
        "Print observation_space and action_space (discrete sizes)",
        "Render and visualise the grid; annotate S, G, H tiles and map indices",
        "Document the reward structure: goal=+1, hole=0, default steps=0; explain stochastic transitions when slippery=True"
      ],
      "files": ["src/describe_env.py", "notebooks/01_explore_env.ipynb"],
      "expected_output": "Human-readable grid with indices and printed action/state space sizes; short markdown explanation",
      "time_estimate_minutes": 40,
      "verification": "Run describe_env.py -> prints grid layout and spaces; notebook shows rendered map cells"
    },
    {
      "id": 3,
      "title": "Implement the Q-learning agent (core)",
      "tasks": [
        "Create Q-table: numpy array shape [n_states, n_actions], initialize to zeros",
        "Implement epsilon-greedy policy: choose random action with probability epsilon, otherwise argmax Q(s,·)",
        "Implement learning step: Q(s,a) = Q(s,a) + alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))",
        "Support epsilon decay schedule (e.g., epsilon = max(epsilon_min, epsilon * decay_rate))",
        "Allow setting hyperparameters via a config or CLI args"
      ],
      "files": ["src/q_learning.py", "src/agent.py"],
      "expected_output": "A reusable QLearningAgent class with methods: act(state), learn(s,a,r,s',done), save(path), load(path)",
      "time_estimate_minutes": 90,
      "verification": "Unit-test: instantiate agent, run a fake step and assert Q-values update as expected"
    },
    {
      "id": 4,
      "title": "Training loop and experiment harness",
      "tasks": [
        "Implement train.py: loop over episodes, each episode run until done or max_steps",
        "Log per-episode total reward, steps, and whether goal reached",
        "Decay epsilon per episode; optionally print progress every N episodes",
        "Store results (CSV/JSON) and periodically save Q-table snapshots",
        "Support seeding for reproducibility: seed env and numpy/random"
      ],
      "files": ["src/train.py", "src/utils.py", "results/"],
      "expected_output": "results/ folder containing training_history.csv, final Q-table file, and logs",
      "time_estimate_minutes": 90,
      "verification": "Run train.py for a small number of episodes (100) and confirm logs and a Q-table file are created"
    },
    {
      "id": 5,
      "title": "Evaluation and metrics",
      "tasks": [
        "Implement evaluate.py to run the learned policy (greedy) for M evaluation episodes with epsilon=0",
        "Compute metrics: success rate (goal reached fraction), average steps for successful episodes, reward per episode distribution",
        "Run multiple seeds to measure variance",
        "Plot learning curves: reward vs episodes, success rate moving average, epsilon over time",
        "Visualise policy: for each state show best action arrow and Q-value heatmap"
      ],
      "files": ["src/evaluate.py", "notebooks/02_evaluate_and_visualize.ipynb"],
      "expected_output": "Numeric evaluation outputs + plots saved to results/plots",
      "time_estimate_minutes": 60,
      "verification": "Run evaluate.py on final Q-table; confirm plots and metrics are generated"
    },
    {
      "id": 6,
      "title": "Hyperparameter tuning and experiments",
      "tasks": [
        "Design experiments for grid/search over alpha, gamma, initial_epsilon, decay_rate, n_episodes",
        "Run experiments and collect summary metrics to find robust hyperparameters",
        "Document how hyperparameters affect exploration vs convergence on slippery maps",
        "Record best-performing configuration(s) and justify selections in the report"
      ],
      "files": ["src/experiments.py", "results/experiments_summary.csv"],
      "expected_output": "A table of hyperparameter combinations + evaluation metrics; recommended hyperparameters",
      "time_estimate_minutes": 120,
      "verification": "Run a short grid search (e.g., 3x3) and confirm results/summary file created"
    },
    {
      "id": 7,
      "title": "Robustness checks and edge cases",
      "tasks": [
        "Test on different map sizes (4x4, 8x8); evaluate difficulties",
        "Test with different random seeds and report variance",
        "Test with changes to slippery parameter (slippery=False) to compare deterministic vs stochastic dynamics",
        "Analyze failure modes: policy that gets stuck, poor exploration, incorrect decay schedules"
      ],
      "files": ["notebooks/03_robustness_checks.ipynb"],
      "expected_output": "Documentation and plots comparing variants, plus a short analysis paragraph",
      "time_estimate_minutes": 90,
      "verification": "Produce a short table: map_size vs success_rate vs stddev"
    },
    {
      "id": 8,
      "title": "Documentation and report preparation",
      "tasks": [
        "Write the report section for Problem 5: background, environment description, implementation details, hyperparameters, experiments, results, and discussion",
        "Include the GitHub link, instructions to run training and evaluation, and explanation of files produced",
        "Add concise README examples with commands for training and evaluating",
        "Bundle results/plots and final Q-table in results/ with clear filenames"
      ],
      "files": ["docs/problem5_report.md", "README.md"],
      "expected_output": "Report-ready artifacts and a short instructions block for graders",
      "time_estimate_minutes": 120,
      "verification": "Open docs/problem5_report.md and confirm it contains required subsections and links to results"
    },
    {
      "id": 9,
      "title": "Optional extensions (extra credit)",
      "tasks": [
        "Implement SARSA and compare performance to Q-learning",
        "Implement function approximation / small DQN for larger maps",
        "Add curriculum training: start non-slippery then gradually increase stochasticity",
        "Implement shaping rewards or reward bonuses for efficiency to reduce steps"
      ],
      "files": ["src/sarsa.py", "src/dqn_small.py"],
      "expected_output": "Optional implementations and comparisons included in an appendix",
      "time_estimate_minutes": 240,
      "verification": "Run each extension for a small experiment and include results in the report appendix"
    }
  ],
  "edge_cases": [
    "Sparse rewards: long episode lengths with few reward signals (use more episodes or reward shaping)",
    "Stochastic transitions: high variance across runs (use more seeds, smoothing in plots)",
    "Exploration collapse: epsilon decays too fast leading to poor policies",
    "State-action ties: deterministic argmax breaks ties — use random tie-breaking to improve exploration"
  ],
  "deliverables": [
    "problem5_plan.json (this file)",
    "Source code in src/ (q_learning, train, evaluate, utils)",
    "requirements.txt and README.md with run instructions",
    "results/ with training history, final Q-table, evaluation metrics, and plots",
    "A section in the combined report describing methods, experiments, and a link to the GitHub repository"
  ],
  "try_it_quick": {
    "note": "Minimal quick test to verify the pipeline works (small run)",
    "steps": [
      "Install requirements",
      "Run: python src/train.py --env FrozenLake-v1 --map 4x4 --episodes 200 --max_steps 100",
      "Run: python src/evaluate.py --qtable results/q_table.npy --episodes 100"
    ]
  },
  "notes": "If using Gym vs Gymnasium APIs differs in your environment, adapt env.make and render calls accordingly. For reproducible results, set seeds for numpy, python's random, and the environment. Recommended hyperparameter starting values: alpha=0.1, gamma=0.99, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, episodes=2000 (increase for harder maps)."
}
